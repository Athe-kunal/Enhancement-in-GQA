{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIM\n",
    "1. Along the head dimension do KNN and order the heads based on the number of groups, that is the most closes cluster is together\n",
    "    1. Getting the neighbours based on similarity and group number\n",
    "    2. Take it to the GPU, by building the index there\n",
    "2. Integrate the logic in the Grouped Query Attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMILARITY BASED GQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEPS\n",
    "1. Go through the architecture and based on encoder, decoder and EncDecoder Attention (cross-attention) get the queries, keys and values (have the attention layer name as attribute)\n",
    "2. Apply KNN and arrange the key, queries and values. Do it in GPU\n",
    "3. Return the model with the shuffled K, Q and V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers.models.t5.modeling_t5 import T5Attention, T5Config, T5Block\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "t5: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "        \"t5-small\"\n",
    "    )\n",
    "\n",
    "tf_attention_list = []\n",
    "transfer_to_gqa: List[str] = [\"encoder\",\"decoder\",\"EncDecAttention\"]\n",
    "def convert_t5_to_gqa(module, kv_heads: int,similarity_flag:bool=False,inplace: bool = False):\n",
    "    \"\"\"Get the list of attention modules based on the flag about encoder, decoder or cross-attention\n",
    "\n",
    "    Args:\n",
    "        module: Transformer module/unit\n",
    "        kv_heads (int): Number of key-value heads\n",
    "        similarity_flag (bool, optional): Similarity GQA flag. Defaults to False.\n",
    "        inplace (bool, optional): inplace replace the model with GQA. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if isinstance(module, T5Attention) and similarity_flag:\n",
    "        # for name, child in module.named_children():\n",
    "        #     print(name)\n",
    "        # print('Module:',module)\n",
    "        # \n",
    "        tf_attention_list.append(module)\n",
    "        # return SimT5GQA.from_t5_attention(module, kv_heads=kv_heads)\n",
    "\n",
    "    out = module if inplace else deepcopy(module)\n",
    "    for name, child in out.named_children():\n",
    "        # if name in [\"encoder\",\"decoder\",\"EncDecAttention\"]:\n",
    "            # print(name,child)\n",
    "        #     print(\"-\"*100)\n",
    "        if name in transfer_to_gqa:\n",
    "            # print(name,child)\n",
    "            similarity_flag = True\n",
    "        out._modules[name] = convert_t5_to_gqa(child, kv_heads=kv_heads,similarity_flag=similarity_flag, inplace=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDEX ONE OF THE MODULE TO DO SIMILARITY-BASED GROUPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = convert_t5_to_gqa(t5,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_attn = tf_attention_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first_attn.q.weight.data\n",
    "first_attn.v.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_attn.n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512//8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLITING THE PROJECTION HEADS INTO RESPECTIVE HEADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# num_heads = first_attn.n_heads\n",
    "# query_heads = torch.tensor_split(first_attn.q.weight,num_heads,dim=1)\n",
    "# key_heads = torch.tensor_split(first_attn.k.weight.data,num_heads,dim=1)\n",
    "# value_heads = torch.tensor_split(first_attn.v.weight.data,num_heads,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query\n",
    "# (n_seq x d_model) @ (d_model x d_model//num_heads) = (n_seq x d_model//num_head)\n",
    "\n",
    "#key\n",
    "# (n_seq x d_model) @ (d_model x d_model//num_heads)  = (n_seq x d_model//num_head)\n",
    "\n",
    "#value\n",
    "# (n_seq x d_model) @ (d_model x d_model//num_heads) =  (n_seq x d_model//num_head)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity(query_heads,key_heads,value_heads):\n",
    "\n",
    "    num_heads = 8\n",
    "    query_heads = torch.tensor_split(query_heads,num_heads,dim=1)\n",
    "    key_heads = torch.tensor_split(key_heads,num_heads,dim=1)\n",
    "    value_heads = torch.tensor_split(value_heads,num_heads,dim=1)\n",
    "\n",
    "    # num_heads = len(query_heads)\n",
    "    flattened_vectors = [head.reshape(-1) for head in query_heads]  # Flatten each matrix\n",
    "    pair_similarities = []\n",
    "\n",
    "    # Calculate cosine similarity for all pairs\n",
    "    for i in range(num_heads):\n",
    "        for j in range(i + 1, num_heads):\n",
    "            vec1 = F.normalize(flattened_vectors[i], p=2, dim=0)\n",
    "            vec2 = F.normalize(flattened_vectors[j], p=2, dim=0)\n",
    "            similarity = torch.dot(vec1, vec2).item()\n",
    "            pair_similarities.append((similarity, i, j))\n",
    "\n",
    "    # Sort pairs by similarity (highest first)\n",
    "    pair_similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    # Group heads into pairs based on highest similarity\n",
    "    grouped_pairs = []\n",
    "    used_heads = set()\n",
    "    for _, head1, head2 in pair_similarities:\n",
    "        if head1 not in used_heads and head2 not in used_heads:\n",
    "            grouped_pairs.append(head1)\n",
    "            grouped_pairs.append(head2)\n",
    "            used_heads.update([head1, head2])\n",
    "    # print(grouped_pairs)\n",
    "    query_heads_grouped = torch.cat([query_heads[i] for i in grouped_pairs],dim=1)\n",
    "    key_heads_grouped = torch.cat([key_heads[i] for i in grouped_pairs],dim=1)\n",
    "    value_heads_grouped = torch.cat([value_heads[i] for i in grouped_pairs],dim=1)\n",
    "\n",
    "    return query_heads_grouped,key_heads_grouped,value_heads_grouped,grouped_pairs\n",
    "\n",
    "# q_grp, k_grp, v_grp  = cosine_similarity(query_heads,key_heads,value_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_attention_list = []\n",
    "transfer_to_gqa: List[str] = [\"decoder\"]\n",
    "def convert_t5_to_gqa(module, kv_heads: int,similarity_flag:bool=True,inplace: bool = False):\n",
    "    \"\"\"Get the list of attention modules based on the flag about encoder, decoder or cross-attention\n",
    "\n",
    "    Args:\n",
    "        module: Transformer module/unit\n",
    "        kv_heads (int): Number of key-value heads\n",
    "        similarity_flag (bool, optional): Similarity GQA flag. Defaults to False.\n",
    "        inplace (bool, optional): inplace replace the model with GQA. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if not similarity_flag:\n",
    "        return module if inplace else deepcopy(module)\n",
    "    \n",
    "    out = module if inplace else deepcopy(module)\n",
    "    \n",
    "    num_heads = 8\n",
    "\n",
    "    for component_name in transfer_to_gqa:\n",
    "        component = getattr(out,component_name)\n",
    "        for layer in component.block:\n",
    "            if component_name == 'encoder':\n",
    "                pass\n",
    "            elif component_name == 'decoder':\n",
    "                decoder_self_attention_block = layer.layer[0].SelfAttention\n",
    "                decoder_cross_attention_block = layer.layer[1].EncDecAttention \n",
    "\n",
    "                tf_attention_list.extend([decoder_self_attention_block,decoder_cross_attention_block])\n",
    "                \n",
    "                # Get the query, key, and value tensors for self-attention\n",
    "                q_lin, k_lin, v_lin = decoder_self_attention_block.q, decoder_self_attention_block.k, decoder_self_attention_block.v\n",
    "                query_heads,q_bias,key_heads,k_bias,value_heads,v_bias = q_lin.weight,q_lin.bias,k_lin.weight,k_lin.bias,v_lin.weight,v_lin.bias\n",
    "\n",
    "                # Reorder them based on cosine similarity\n",
    "                query_heads,key_heads,value_heads,grouped_pairs = cosine_similarity(query_heads, key_heads, value_heads)\n",
    "                # print(grouped_pairs)\n",
    "                # Replace original tensors with reordered ones\n",
    "                decoder_self_attention_block.q.weight = torch.nn.Parameter(query_heads) \n",
    "                decoder_self_attention_block.k.weight = torch.nn.Parameter(key_heads)\n",
    "                decoder_self_attention_block.v.weight = torch.nn.Parameter(value_heads)\n",
    "\n",
    "                #this condition needs to be checked if bias is being used, not sure if this works\n",
    "                if q_bias is not None:\n",
    "                    q_bias = q_bias[grouped_pairs]\n",
    "                    k_bias = k_bias[grouped_pairs]\n",
    "                    v_bias = v_bias[grouped_pairs]\n",
    "\n",
    "                    decoder_self_attention_block.q.bias = torch.nn.Parameter(q_bias)\n",
    "                    decoder_self_attention_block.k.bias = torch.nn.Parameter(k_bias)\n",
    "                    decoder_self_attention_block.v.bias = torch.nn.Parameter(v_bias)\n",
    "\n",
    "                # Get the query, key, and value tensors for cross-attention\n",
    "                q_cross_lin, k_cross_lin, v_cross_lin = decoder_cross_attention_block.q, decoder_cross_attention_block.k, decoder_cross_attention_block.v\n",
    "                query_heads,q_bias,key_heads,k_bias,value_heads,v_bias = q_cross_lin.weight,q_cross_lin.bias,k_cross_lin.weight,k_cross_lin.bias,v_cross_lin.weight,v_cross_lin.bias\n",
    "\n",
    "                # Reorder them based on cosine similarity\n",
    "                query_heads,key_heads,value_heads,grouped_pairs = cosine_similarity(query_heads, key_heads, value_heads)\n",
    "                \n",
    "                # Replace original tensors with reordered ones\n",
    "                decoder_cross_attention_block.q.weight = torch.nn.Parameter(query_heads) \n",
    "                decoder_cross_attention_block.k.weight = torch.nn.Parameter(key_heads)\n",
    "                decoder_cross_attention_block.v.weight = torch.nn.Parameter(value_heads)\n",
    "\n",
    "                #this condition needs to be checked if bias is being used, not sure if this works\n",
    "                if q_bias is not None:\n",
    "                    q_bias = q_bias[grouped_pairs]\n",
    "                    k_bias = k_bias[grouped_pairs]\n",
    "                    v_bias = v_bias[grouped_pairs]\n",
    "\n",
    "                    decoder_cross_attention_block.q.bias = torch.nn.Parameter(q_bias)\n",
    "                    decoder_cross_attention_block.k.bias = torch.nn.Parameter(k_bias)\n",
    "                    decoder_cross_attention_block.v.bias = torch.nn.Parameter(v_bias)\n",
    "\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_model = convert_t5_to_gqa(t5,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS KNN WITH GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "b = 32\n",
    "h = 8\n",
    "n = 256\n",
    "d = 512\n",
    "\n",
    "vals = torch.randn(d,d, device=\"cuda\", dtype=torch.float16)\n",
    "split_vals = torch.tensor_split(vals,h,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(split_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(split_vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss_knn_wrapper import FaissKNNClassifier\n",
    "\n",
    "fknn = FaissKNNClassifier(3,device=\"cuda\",n_cells=4,algorithm=\"voronoi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_of_tensors_to_tensor(tuple_of_tensors):\n",
    "    return  torch.stack(list(tuple_of_tensors), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_vals = tuple_of_tensors_to_tensor(split_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong number or type of arguments for overloaded function 'new_GpuIndexIVFFlat'.\n  Possible C/C++ prototypes are:\n    faiss::gpu::GpuIndexIVFFlat::GpuIndexIVFFlat(faiss::gpu::GpuResourcesProvider *,faiss::IndexIVFFlat const *,faiss::gpu::GpuIndexIVFFlatConfig)\n    faiss::gpu::GpuIndexIVFFlat::GpuIndexIVFFlat(faiss::gpu::GpuResourcesProvider *,faiss::IndexIVFFlat const *)\n    faiss::gpu::GpuIndexIVFFlat::GpuIndexIVFFlat(faiss::gpu::GpuResourcesProvider *,int,int,faiss::MetricType,faiss::gpu::GpuIndexIVFFlatConfig)\n    faiss::gpu::GpuIndexIVFFlat::GpuIndexIVFFlat(faiss::gpu::GpuResourcesProvider *,int,int,faiss::MetricType)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/athekunal/GQA/grouped-query-attention-pytorch/similarity_grouped_query_attention_pytorch/test.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/athekunal/GQA/grouped-query-attention-pytorch/similarity_grouped_query_attention_pytorch/test.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m fknn\u001b[39m.\u001b[39;49mfit(fit_vals)\n",
      "File \u001b[0;32m~/GQA/grouped-query-attention-pytorch/similarity_grouped_query_attention_pytorch/faiss_knn_wrapper.py:134\u001b[0m, in \u001b[0;36mFaissKNNClassifier.fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    132\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    133\u001b[0m d \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]  \u001b[39m# dimensionality of the feature vector\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_knn_algorithm(X, d)\n\u001b[1;32m    135\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_\u001b[39m.\u001b[39madd(X)\n\u001b[1;32m    136\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/GQA/grouped-query-attention-pytorch/similarity_grouped_query_attention_pytorch/faiss_knn_wrapper.py:152\u001b[0m, in \u001b[0;36mFaissKNNClassifier._prepare_knn_algorithm\u001b[0;34m(self, X, d)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvoronoi\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    151\u001b[0m     quantizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfaiss\u001b[39m.\u001b[39mGpuIndexFlatL2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres,d,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig)\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfaiss\u001b[39m.\u001b[39;49mGpuIndexIVFFlat(quantizer, d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_cells)\n\u001b[1;32m    153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_\u001b[39m.\u001b[39mtrain(X)\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_\u001b[39m.\u001b[39mnprobe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_probes\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/faiss/swigfaiss.py:9443\u001b[0m, in \u001b[0;36mGpuIndexIVFFlat.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   9425\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m   9426\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   9427\u001b[0m \u001b[39m    *Overload 1:*\u001b[39;00m\n\u001b[1;32m   9428\u001b[0m \u001b[39m    Construct from a pre-existing faiss::IndexIVFFlat instance, copying\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9441\u001b[0m \u001b[39m    provides the number of lists desired.\u001b[39;00m\n\u001b[1;32m   9442\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 9443\u001b[0m     _swigfaiss\u001b[39m.\u001b[39mGpuIndexIVFFlat_swiginit(\u001b[39mself\u001b[39m, _swigfaiss\u001b[39m.\u001b[39;49mnew_GpuIndexIVFFlat(\u001b[39m*\u001b[39;49margs))\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrong number or type of arguments for overloaded function 'new_GpuIndexIVFFlat'.\n  Possible C/C++ prototypes are:\n    faiss::gpu::GpuIndexIVFFlat::GpuIndexIVFFlat(faiss::gpu::GpuResourcesProvider *,faiss::IndexIVFFlat const *,faiss::gpu::GpuIndexIVFFlatConfig)\n    faiss::gpu::GpuIndexIVFFlat::GpuIndexIVFFlat(faiss::gpu::GpuResourcesProvider *,faiss::IndexIVFFlat const *)\n    faiss::gpu::GpuIndexIVFFlat::GpuIndexIVFFlat(faiss::gpu::GpuResourcesProvider *,int,int,faiss::MetricType,faiss::gpu::GpuIndexIVFFlatConfig)\n    faiss::gpu::GpuIndexIVFFlat::GpuIndexIVFFlat(faiss::gpu::GpuResourcesProvider *,int,int,faiss::MetricType)\n"
     ]
    }
   ],
   "source": [
    "fknn.fit(fit_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fit_vals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import MultiheadGQA\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum, rearrange\n",
    "from torch import Tensor, nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiheadGQA(\n",
    "    embed_dim=512, query_heads=8, kv_heads=2, device=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=128, bias=True)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha.k_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 512])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = torch.randn(1, 256, 512)\n",
    "\n",
    "q = nn.Linear(512,512)\n",
    "q(query).shape\n",
    "# 1x256x512\n",
    "# 1 x 256 x 128 - k\n",
    "# 1 x 256 x 128 - v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shapes: (batch_size, seq_len, embed_dim)\n",
    "query = torch.randn(1, 256, 512)\n",
    "key = torch.randn(1, 256, 512)\n",
    "value = torch.randn(1, 256, 512)\n",
    "\n",
    "# out, attn_weights = mha(\n",
    "#     query,\n",
    "#     key,\n",
    "#     value,\n",
    "#     is_causal=True, # default: False\n",
    "#     need_weights=True, # default: False, which returns 'attn_weights=None'\n",
    "# )\n",
    "# print(out.shape)  # (batch_size, q_seq_len, embed_dim)\n",
    "# # torch.Size([1, 256, 512])\n",
    "# print(attn_weights.shape)  # (batch_size, q_seq_len, kv_seq_len, kv_heads)\n",
    "# # torch.Size([1, 256, 128, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256, 512]),\n",
       " torch.Size([1, 256, 128]),\n",
       " torch.Size([1, 256, 128]))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = mha.q_proj(query)\n",
    "key = mha.k_proj(key)\n",
    "value = mha.v_proj(value)\n",
    "\n",
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha.kv_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = rearrange(query, \"b n (h d) -> b n h d\", h=8)\n",
    "k = rearrange(key, \"b n (h d) -> b n h d\", h=2)\n",
    "v = rearrange(value, \"b n (h d) -> b n h d\", h=mha.kv_heads)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256, 8, 64]),\n",
       " torch.Size([1, 256, 2, 64]),\n",
       " torch.Size([1, 256, 2, 64]))"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 256, 64]),\n",
       " torch.Size([1, 2, 256, 64]),\n",
       " torch.Size([1, 2, 256, 64]))"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = rearrange(q, \"b n h d -> b h n d\")\n",
    "key = rearrange(k, \"b s h d -> b h s d\")\n",
    "value = rearrange(v, \"b s h d -> b h s d\")\n",
    "\n",
    "bq, hq, nq, dq = query.shape\n",
    "bk, hk, nk, dk = key.shape\n",
    "bv, hv, nv, dv = value.shape\n",
    "\n",
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2, 256, 64])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_head_groups = hq // hk\n",
    "query1 = rearrange(query, \"b (h g) n d -> b g h n d\", g=num_head_groups)\n",
    "query1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 256, 64]), torch.Size([1, 2, 256, 64]))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256, 256])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = einsum(query1, key, \"b g h n d, b h s d -> b h n s\")\n",
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256, 64])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " scale=query.size(-1) ** 0.5\n",
    " attention = F.softmax(similarity / scale, dim=-1)\n",
    " out = einsum(attention, value, \"b h n s, b h s d -> b h n d\")\n",
    " out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 128])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rearrange(out, \"b h n d -> b n h d\")\n",
    "# out.shape\n",
    "x = rearrange(out, \"b n h d -> b n (h d)\")\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 512])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = mha.out_proj(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    " from attention import scaled_dot_product_gqa\n",
    " x, attn = scaled_dot_product_gqa(\n",
    "            query=q,\n",
    "            key=k,\n",
    "            value=v,\n",
    "            is_causal=True,\n",
    "            need_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes: (batch_size, seq_len, num_heads, head_dim)\n",
    "query = torch.randn(1, 256, 8, 64)\n",
    "key = torch.randn(1, 256, 2, 64)\n",
    "value = torch.randn(1, 256, 2, 64)\n",
    "\n",
    "out, attn_weights = scaled_dot_product_gqa(\n",
    "    query,\n",
    "    key,\n",
    "    value,\n",
    "    is_causal=True,  # default: False\n",
    "    need_weights=True,  # default: False, which returns 'attn_weights=None'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'T5ForConditionalGeneration' object has no attribute 'is_decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32864\\4274470357.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_decoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\saise\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'T5ForConditionalGeneration' object has no attribute 'is_decoder'"
     ]
    }
   ],
   "source": [
    "t5.is_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
