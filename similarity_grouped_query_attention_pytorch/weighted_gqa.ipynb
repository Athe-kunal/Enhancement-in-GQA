{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer,T5Config\n",
    "from transformers.models.t5.modeling_t5 import T5Attention, T5Config, T5Block\n",
    "from copy import deepcopy\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.t5.modeling_t5 import T5Attention, T5LayerSelfAttention, T5LayerCrossAttention\n",
    "\n",
    "t5: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_tf_attention_dict(module,kv_heads:int=4):\n",
    "    transfer_to_gqa: List[str] = [\"decoder\",\"EncDecAttention\"]\n",
    "    tf_attention_dict = defaultdict(list)\n",
    "    def convert_t5_to_gqa(module, kv_heads: int,similarity_flag:bool=False,inplace: bool = False,curr_name:str=''):\n",
    "        \"\"\"Get the list of attention modules based on the flag about encoder, decoder or cross-attention\n",
    "\n",
    "        Args:\n",
    "            module: Transformer module/unit\n",
    "            kv_heads (int): Number of key-value heads\n",
    "            similarity_flag (bool, optional): Similarity GQA flag. Defaults to False.\n",
    "            inplace (bool, optional): inplace replace the model with GQA. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if isinstance(module, T5Attention) and similarity_flag:\n",
    "            print(curr_name)\n",
    "            tf_attention_dict[curr_name].append(module)\n",
    "\n",
    "        out = module if inplace else deepcopy(module)\n",
    "        for name, child in out.named_children():\n",
    "            if name in transfer_to_gqa:\n",
    "                curr_name = name\n",
    "                similarity_flag = True\n",
    "            out._modules[name] = convert_t5_to_gqa(child, kv_heads=kv_heads,similarity_flag=similarity_flag, inplace=True,curr_name=curr_name)\n",
    "        return out\n",
    "\n",
    "    convert_t5_to_gqa(module,kv_heads=kv_heads)\n",
    "    return tf_attention_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder\n",
      "EncDecAttention\n",
      "decoder\n",
      "EncDecAttention\n",
      "decoder\n",
      "EncDecAttention\n",
      "decoder\n",
      "EncDecAttention\n",
      "decoder\n",
      "EncDecAttention\n",
      "decoder\n",
      "EncDecAttention\n"
     ]
    }
   ],
   "source": [
    "tf_attn_dict = get_tf_attention_dict(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['decoder', 'EncDecAttention'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_attn_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_attn_dict['EncDecAttention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Attention(\n",
       "  (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (relative_attention_bias): Embedding(32, 8)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_qkv = tf_attn_dict['decoder'][0]\n",
    "first_qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = first_qkv.q.weight.data.T\n",
    "k = first_qkv.k.weight.data.T\n",
    "v = first_qkv.v.weight.data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 512]), torch.Size([512, 512]), torch.Size([512, 512]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape,q.shape,k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "#define 8 weights one for each key head and 8 weights one for each value head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECODER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming your original matrix is named 'input_matrix' with dimensions (512, 512)\n",
    "input_matrix = torch.rand((512, 512))\n",
    "\n",
    "# Define the weight scalars w1, w2, ..., w8\n",
    "w_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "# Reshape the input matrix to (512, 8, 64) to represent the 8 heads\n",
    "input_matrix_reshaped = input_matrix.view(512, 8, 64)\n",
    "\n",
    "# Multiply each head with its corresponding weight scalar\n",
    "output_heads = torch.stack([input_matrix_reshaped[:, i, :] * w_values[i] for i in range(8)], dim=1)\n",
    "\n",
    "# Sum the weighted heads along the second dimension to combine them\n",
    "output_matrix = output_heads.sum(dim=1)\n",
    "\n",
    "# Verify the shape of the resulting matrix (512, 512)\n",
    "print(output_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0.2000, 0.2000, 0.3000, 0.3000],\n",
      "        [0.2000, 0.2000, 0.3000, 0.3000],\n",
      "        [0.2000, 0.2000, 0.3000, 0.3000],\n",
      "        [0.2000, 0.2000, 0.3000, 0.3000]])\n"
     ]
    }
   ],
   "source": [
    "w = torch.ones((4,4))\n",
    "print(w)\n",
    "w = w.view(4,2,2)\n",
    "mul_val = torch.tensor([[0.2],[0.3]])\n",
    "# print(mul_val.shape)\n",
    "# w_1 = torch.reshape(w,())\n",
    "final_val = torch.multiply(w,mul_val)\n",
    "print(final_val.view(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "tensor_0_5 = torch.full((8, 1), 0.5)\n",
    "\n",
    "# Print the resulting tensor\n",
    "print(tensor_0_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn((8,1))\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,attn_module in tf_attn_dict['decoder']:\n",
    "    q = attn_module.q.weight.data.T\n",
    "    k = attn_module.k.weight.data.T\n",
    "    v = attn_module.v.weight.data.T\n",
    "\n",
    "    params = nn.ParameterDict({\n",
    "        f\"key_{idx}\": nn.Parameter(torch.full((num_heads,1),0.5)),\n",
    "        f\"value_{idx}\": nn.Parameter(torch.full((num_heads,1),0.5)),\n",
    "    })\n",
    "\n",
    "    k = k.view(512,num_heads,512//num_heads)\n",
    "    v = v.view(512,num_heads,512//num_heads)\n",
    "    k_mod = torch.multiply(k,params[f\"key_{idx}\"])\n",
    "    v_mod = torch.multiply(v,params[f\"value_{idx}\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b: batch_size\n",
    "# @n: seq_length\n",
    "# d: d_model\n",
    "# h:heads\n",
    "# k: same as d_model\n",
    "                #16,\n",
    "Q= tf .einsum(\"bnd,hdk->bhnk\" , X, P_q) \n",
    "K= tf .einsum(\"bmd,dk->bmk\" , M, P_k) \n",
    "V= tf .einsum(\"bmd,dv->bmv\" , M, P_v) \n",
    "logits = tf .einsum(\"bhnk,bmk->bhnm\", Q, K) \n",
    "weights= tf .softmax(logits +mask) \n",
    "O= tf .einsum(\"bhnm,bmv->bhnv\", weights , V)\n",
    "Y= tf .einsum(\"bhnv,hd->bnd\" , O, P_o) \n",
    "# return Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
